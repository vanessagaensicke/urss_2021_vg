{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python for Data Science Project Session 5: Economics and Finance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session, we will look at two mini projects related to Economics/Finance, applying techniques covered in Session Five and extending these a bit further. To start, we will import our general data science packages here, and then add the necessary machine learning imports as we go along so that it's clear when to use what: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing key data science libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convergence warning disabling\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from warnings import simplefilter\n",
    "\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-Projects\n",
    "> [Classification: Which companies go bust?](#Classification:-Which-companies-go-bust?)\n",
    ">\n",
    "> [Regression: What are the drivers of worker productivity?](#Regression:-What-are-the-drivers-of-worker-productivity?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification: Which companies go bust?\n",
    "For this project, we will aim to predict if a company is likely to go bankrupt given a variety of accounting and financial metrics. To do this, we will use  data collected from the Taiwan Economic Journal for the years 1999 to 2009. This kind of model would have all sorts of real world applications, from portfolio stock picks to loan approvals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data set overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start out, lets get an overview of our data set dimensions and feature types. We'll be doing a very simple overview in this section but feel free to perform more extensive EDA on your own: the more you know about your data, the better you can harness it through modelling.\n",
    "\n",
    "Start out with the following tasks:\n",
    "- Read in the data set as a pandas DataFrame (file path: `data/bankruptcy.csv`)\n",
    "- Output the first five data set rows\n",
    "- Print out a comprehensive summary of the data set (dimensions, variable names, data types, and null values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read bankruptcy data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the first five rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the data set overview (dimensions, variable names, data types, null values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our ML setup\n",
    "To start our project we want to set up our data in the usual machine learning configuration: a training and a test set, each with its own feature matrix and target vector. To start:\n",
    "\n",
    "- Import the `train_test_split()` function from scikit learn\n",
    "- Declare a target vector y (corresponding to the `Bankrupt` column in the data set)\n",
    "- Declare a feature matrix X (including all columns except `Bankrupt`)\n",
    "- Create a training and test set\n",
    "- Print the dimensions of X and y in both the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing train-test split function\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the target vector y which has the column label 'Bankrupt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare feature matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a train-test- split using random state 253, test_size 30%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train dimensions: ', X_train.shape)\n",
    "print('y_train dimensions: ', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_test dimensions: ', X_test.shape)\n",
    "print('y_test dimensions: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One problem that is immediately apparent is that we have a high number of features in our data. This constitutes a problem as capturing the complexities of a highly dimensional space through our model can lead us to **overfitting**. \n",
    "> This is known as the [Curse of Dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality#Machine_Learning) and is definitely a topic worth exploring further.\n",
    "\n",
    "\n",
    "In order to cope with the high dimensionality of our data we can employ several dimensionality reduction techniques. The first one we'll explore is called **Principal Component Analysis (PCA)** and for a more in-depth treatment of the subject be sure to refer [here](https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c). PCA essentially works by projecting our data points (which can be seen as vectors) into a lower dimensional space while maximizing the conserved variance of our data.\n",
    "\n",
    "Try out PCA on the training feature matrix `X_train` below:\n",
    "- Import the `PCA` class from the `sklearn.decomposition` module.\n",
    "- Decide on a number for `n_components`. This is going to be the new number of features.\n",
    "- Create a PCA transformer.\n",
    "- Train the transformer using the `.fit()` method.\n",
    "- Apply dimensionality reduction using the `.transform()` method.\n",
    "- Output the first five rows.\n",
    "\n",
    "> **Note:** The `.tranfrom()` method will return a NumPy array as opposed to a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing PCA class\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PCA transformer that keeps 30 components \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit and transform using our PCA model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might've noticed that most machine learning with scikit learn involves:\n",
    "- Instantiating a transformer or model.\n",
    "- Fitting this to our data using `.fit()`.\n",
    "- Transforming or predicting using `.transform()` or `.predict()`.\n",
    "\n",
    "This simple, general process means that we can simplify our ML process using **Pipelines**. A Pipeline allows you to establish a sequence of transformations terminating with *one estimator*. Pipelines can then be treated as any other scikit learn estimator, but each time they perform all of the interim transformation steps in the sequence. This has the benefit of greatly simplifying your code and can also help prevent [data leakage](https://machinelearningmastery.com/data-leakage-machine-learning/). \n",
    "\n",
    "To instantiate a Pipeline you must pass as an argument a list of $n+1$ tuples (where $n$ is the number of transforms) in the form: `[('transformer_1_name', transformer_1), ..., ('estimator_name', estimator)]`.\n",
    "\n",
    "We will now create Pipelines for a KNN, SVM, and Logistic Regression model below:\n",
    "- Import the `StandardScaler`, `KNeighborsClassifier`, `SVC`, and `LogisticRegression` classes from scikit learn.\n",
    "- Import the `Pipeline` class from scikit learn.\n",
    "- For each one of the three models, create a Pipeline with the following transforms (in order): scaling, PCA, estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing KNN, SVM, and Logistic Regression models \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine these into three Pipelines, one for each model\n",
    "knn_pipe = Pipeline([('Scaler', StandardScaler()), \n",
    "                     ('PCA', PCA(n_components=30)), \n",
    "                     ('Model', KNeighborsClassifier())])\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During Session Five, you were introduced to Accuracy as an evaluation metric for classification model. As a reminder, accuracy is defined as follows:\n",
    "$$ \\text{Accuracy} = \\frac{TP+TN}{TP+FP+TN+FN}$$\n",
    "In other words, it is the ratio of correctly classified samples to the overall number of samples. We're now gonna compute accuracy for our three models. To do this, we use the `cross_val_score()` function from scikit learn (for documentation, see [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html?highlight=cross_val_score#sklearn.model_selection.cross_val_score)). This function calculates the average score (taking accuracy as the default metric for classification models) after carrying out K-fold cross validation, which we prefer in order to reduce bias and thus get a more 'realistic' estimate of our model's performance. \n",
    "\n",
    "To start out, print the average accuracy score for each of our models:\n",
    "- Import the `cross_val_score()` function from scikit learn.\n",
    "- Calculate the cross-validated scores for each pipeline estimator.\n",
    "- Print the mean score for each pipeline estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing cross_val_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating mean cross-validated scores\n",
    "knn_score = cross_val_score(knn_pipe, X_train, y_train).mean()\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('KNN Accuracy Score:')\n",
    "print(knn_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SVM Accuracy Score:')\n",
    "print(svm_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Logistic Regression Accuracy Score:')\n",
    "print(log_reg_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although accuracy is a good starting point to illustrate our model's predictive performance, it does come with some caveats. This is perhaps better illustrated by an example:\n",
    "\n",
    ">**Example:**\n",
    ">\n",
    "> *Imagine if I told you I can build a machine learning model that predicts whether or not an applicant is admitted into Harvard with 96.4% accuracy. Sounds good right! Now what if I told you the model works by predicting `NOT_ACCEPTED` to every applicant, irrespective of any data observed. This works as Harvard has a 4.6% admissions rate, but is our model any good?*  \n",
    "\n",
    "Often, we must consider using other metrics in classification. The best way to start out is by analysing how true positives/negatives are distributed among classes. We can do this through a confusion matrix, which represents this distribution in the following form:\n",
    "\n",
    "\n",
    "<img src='https://miro.medium.com/max/2102/1*fxiTNIgOyvAombPJx5KGeA.png' width=\"450\" height=\"400\">\n",
    "\n",
    "\n",
    "To output the confusion matrix of our predictions, we use the `confusion_matrix()` function from scikit learn (for documentation, see [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html?highlight=confusion#sklearn.metrics.confusion_matrix)). Try outputting the confussion matrix for all three of our model predictions:\n",
    "\n",
    "- Import the `confusion_matrix()` function from scikit learn.\n",
    "- Further separate the training set into training and validation sets using `train_test_split()` (suggested`test_size=0.2`).\n",
    "- Fit all pipeline estimators on the training set and predict on the validation set.\n",
    "- For each estimator's predictions, print the confusion matrix generated with `confusion_matrix()` by passing both `y_val` and `y_pred`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing confusion matrix\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create validation sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train dimensions: ', X_train.shape)\n",
    "print('y_train dimensions: ', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_val dimensions: ', X_val.shape)\n",
    "print('y_val dimensions: ', y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and predict KNN pipeline\n",
    "\n",
    "print('KNN Confusion Matrix:')\n",
    "print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and predict SVM pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and predict Logistic Regression pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our confusion matrix, we can then derive the following useful metrics:\n",
    "\n",
    "\n",
    "$$\\text{Precision} = \\frac{TP}{TP+FP} \\\\\n",
    "\\text{Recall} = \\frac{TP}{TP+FN}\\\\\n",
    "\\text{F1 Score} = 2 * \\frac{\\text{Precision} * \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n",
    "\n",
    "\n",
    "One way to obtain all of these metrics is using the `classification_report()` function from scikit learn (for documentation, see [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html?highlight=classification#sklearn.metrics.classification_report)). Try outputting the classification report for our three models below:\n",
    "\n",
    "- Import the `classification_report()` function from scikit learn.\n",
    "- Fit all pipeline estimators on the training set and predict on the validation set.\n",
    "- For each estimator's predictions, print the confusion matrix generated with `classification_report()` by passing both `y_val` and `y_pred`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing classification_report\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and predict KNN pipeline\n",
    "# ...\n",
    "print('KNN Classification Report:')\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and predict SVM pipeline\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and predict Logistic Regression pipeline\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class imbalance and resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from our classification reports, one metric that is especially low for all three models is recall amongst bankrupt companies. This highlights a frequent problem in classification ML which is known as **class imbalance**. It occurs when one class is significantly underrepresented in the training data, and thus our models are trained really well at \"spotting\" the majority class (which is easy), but really bad at spotting the minority one. This is problematic as our interest is often to predict the minority class (i.e bankrupt companies).\n",
    "\n",
    "To illustrate imbalance in our training data set, have a look at the plot below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ploting class imbalance\n",
    "fig, ax = plt.subplots(figsize=(6, 6)) \n",
    "ax.bar([0,1],y_train.value_counts())\n",
    "ax.set_xlabel('Bankrupt')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_xticks([0,1])\n",
    "ax.set_title('Distribution of company bankruptcy ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve our model's ability to pick up bankrupt companies, we must resample our data set. There are generally two ways to do this:\n",
    "- Undersampling the majority class.\n",
    "- Oversampling the minority class.\n",
    "\n",
    "Here, we'll take a look at the latter option using a technique called Synthetic Minority Over-sampling Technique (SMOTE). For a more in depth treatment of this technique please refer [here](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/).\n",
    "\n",
    "To apply SMOTE, we'll have to use another library called Imbalanced Learn (imported as `imblearn`). To start, lets see how SMOTE resampling changes our class distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing SMOTE class from Imbalanced Learn\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling data\n",
    "sm = SMOTE(random_state=243)\n",
    "X_res, y_res = sm.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAGDCAYAAADaszzmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAeXUlEQVR4nO3df7RdZX3n8ffHoEArKAyRYoKGajoVmBpLymBtO1qdSq1d4HTUaKvUoaal2Gq1jlDtFFtT7aq/6kzFQWX4IRVj1YICVUQRXUVisOG3aCoIIZREqBpapSZ854/zRI43597nBnLuvcl9v9Y66+zz7L2f8z37nHs/Zz97331TVUiSNJWHzXYBkqS5z7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYTEPJHlPkj/eRX09Lsm9SRa0x5cn+a1d0Xfr75IkJ+yq/nbied+U5JtJ/nmmn3uuSVJJnjiGfp+eZMOu7lczY6/ZLkAPTZJbgYOBrcA24EbgHOCMqrofoKp+Zyf6+q2q+vRky1TVbcAjH1rVP3i+04AnVtVvDPX/y7ui752s41DgNcDjq2rTTD+/xiNJAUurav1s17IncM9iz/CrVbUf8HjgLcDrgPfv6idJsqd+uXg8cLdBMXfswZ+13ZZhsQepqm9X1YXAC4ETkhwJkOSsJG9q0wcl+USSbyW5J8nnkzwsybnA44CPt2Gm/5lkSRuSODHJbcBnhtqGf5ifkGRNkm8nuSDJge25dhh2SHJrkmclORb4I+CF7fmuafN/MKzV6npDkm8k2ZTknCSPavO213FCktvaENLrJ9s2SR7V1t/c+ntD6/9ZwKXAY1sdZ02y/nFJ1iX5TpJ/avWT5LFJLmzbcn2Slw+tc1qSDyf5QJItSa5L8hNJTm2v5/YkvzS0/OVJ3jxqW7b5H07yz23eFUmOGJp3VpK/TnJRe66rkjyhzfvrJG+b8Ho+nuRVk20v4DlJvt62618meVhb7wlJPpPk7jbvvCSPnvD+/mGSa1udH0qyzyTb9PeT3Jhk8fbPSpLXZTAU+P+S/GaSL0xY5wdDZO01vyfJpe01fy7J49u8K9oq17T39YWTvY9Jnp/k6gnP85okfzfF9pl/qsrbbnwDbgWeNaL9NuCkNn0W8KY2/WbgPcDD2+3ngYzqC1gCFINhrR8F9h1q26stczlwB3BkW+YjwAfavKcDGyarFzht+7JD8y9nMBQG8D+A9cCPMxj6+ihw7oTa3tvqejJwH/CkSbbTOcAFwH5t3a8CJ05W54R1jwa+DfxXBl+wFgE/2eZ9Dng3sA+wDNgMPHPo9X0PeDaDId9zgFuA17dt/3LglgmvfeS2HNoe+wF7A+8E1g3NOwu4p9W6F3AecP5Q/RuBh7XHBwH/Bhw8yest4LPAgQy+QHx16D15YtsOewMLgSuAd054f9cAj23r3wT8zsTtDPwx8GVg4dC8rcBftL73BX4T+MKI2p449Jq3AL/Q1vmr4eWHl53qfWzr3sPQZwf4R+DXZvvney7d3LPYc21k8MM60feBQxiMz3+/qj5f7adjCqdV1b9W1XcnmX9uVV1fVf/K4JfAC9IOgD9Evw68vaq+XlX3AqcCK/LDezVvrKrvVtU1wDUMQuOHtFpeCJxaVVuq6lbgbcBLplnHicCZVXVpVd1fVXdU1VcyONbxc8Drqup7VbUOeN+Efj9fVZ+sqq3Ahxn8gn1LVX0fOB9YMvzNnCm2ZVWd2eq/j0EQPXn7nlbz0apa057rPAbhRVWtYfBL8pltuRXA5VV11xSv+S+q6p4aHKN6J/Ci1tf6th3uq6rNwNuB/zJh3XdV1caqugf4+PY6miR5O4MAfUbrY7v7gT9pfU/2WZvooqq6om2T1wNPbe/LKCPfx7buh4DfaAUeweALxSemWcO8YFjsuRYx+LY00V8y+Lb+qTbMcMo0+rp9J+Z/g8G35oOmVeXUHtv6G+57LwYH9LcbPnvp3xh98P0g4BEj+lo0zToOBf5pkvruqaotU/Q7/Av5u8A3q2rb0GMm1DxyWyZZkOQtbejkOwy+wcMPb+eptsXZtF+G7f7cEa9n2MQ6HguQ5DFJzk9yR6vjA+z4Xk9Vx6OBlcCbq+rbE9bbXFXf69Q1aZ3tC8U922sdYbL3EQbb58VJwiDsV7cQUWNY7IGS/AyDX1hfmDivfTN9TVX9OPCrwKuTbP/GOdkeRm/PY/ib3OMY7L18E/hX4EeG6lrA4Jv1dPvdyODg83DfW/nhX8DT8c1W08S+7pjm+rcDT5ikvgOT7Pcg+x1lsm35YuA44FnAoxh88wXINPv9AHBckicDTwL+bifr2Nim38zgffupqtqfQfBMtwaAfwGey+CYxNMmzJv4eZj4+fmxqepM8kgGe9MbRywHk7+PVNUXgX9nMCz7YvphOu8YFnuQJPsneS6D4Y0PVNV1I5Z5bpIntm9Q32Fwuu32b7p3MTg+sLN+I8nhSX4E+FPgb9u3568C+yT5lSQPB97AYHx4u7sYDMNM9jn8IPAHSQ5rvwj+HPhQG2aZtlbLamBVkv3aQdBXM/gFOh3vB16W5JkZHBRflOQnq+p24B+ANyfZJ8lPMRjqOG9n6ptgsm25H4NjMncz+AX65zvTaVVtAL7E4JfgR6YxzPPaJAe0IZ1XMhimodVxL/CtJIuA1+5MHa2WyxkMMX4syX+eYtFrgCOSLGsHyU8bscxzkvxckkcAfwZc1d4X2PHzPPJ9HJp/DvB/gK1VtcMXrfnOsNgzfDzJFgbfnF7PYBz5ZZMsuxT4NIMf+CuBd7cfXhh8a3xDBmdK/eFOPP+5DA42/jODA72/D4Ozs4DfZTCOfweDb4rDZ0d9uN3fneTLI/o9s/V9BYMDw98Dfm8n6hr2e+35v85gj+tvWv9dbcz/ZcA7GIz9f44H9lJexOBb/kbgYwzG3C99kDXCJNuSwS+ybzDYjjcCX3wQfZ8N/Cem9635AuBqYB1wEQ+civ1G4KcZbIeLGJx0sNPaNnoZcGGSoyZZ5qsMAvPTwNcYsafM4H38EwbDT0cxCKHtTgPObp/nF3TeRxhslyNxr2Kk7WfBSJplSS5nsEf4vjH1/wsM9qaWVPuDzd1ZBqc5b6iqN+yi/vYFNgE/XVVf2xV97kncs5DmgTYM+ErgfXtCUIzJScCXDIrR/CtJaQ+X5EnAWgbHACYbnpzXMrjUTYDjZ7eSucthKElSl8NQkqQuw0KS1LXHHrM46KCDasmSJbNdhiTtVq6++upvVtXCie17bFgsWbKEtWvXznYZkrRbSfKNUe0OQ0mSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKlrj73q7EOx5JSLZrsEzVG3vuVXZrsEwM+oJjeuz6h7FpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lS19jCIsk+SdYkuSbJDUne2NpPS3JHknXt9pyhdU5Nsj7JzUmePdR+VJLr2rx3Jcm46pYk7WicV529D/jFqro3ycOBLyS5pM17R1W9dXjhJIcDK4AjgMcCn07yE1W1DTgdWAl8EbgYOBa4BEnSjBjbnkUN3NsePrzdaopVjgPOr6r7quoWYD1wdJJDgP2r6sqqKuAc4Phx1S1J2tFYj1kkWZBkHbAJuLSqrmqzXpHk2iRnJjmgtS0Cbh9afUNrW9SmJ7ZLkmbIWMOiqrZV1TJgMYO9hCMZDCk9AVgG3Am8rS0+6jhETdG+gyQrk6xNsnbz5s0PsXpJ0nYzcjZUVX0LuBw4tqruaiFyP/Be4Oi22Abg0KHVFgMbW/viEe2jnueMqlpeVcsXLly4a1+EJM1j4zwbamGSR7fpfYFnAV9pxyC2ex5wfZu+EFiRZO8khwFLgTVVdSewJckx7SyolwIXjKtuSdKOxnk21CHA2UkWMAil1VX1iSTnJlnGYCjpVuC3AarqhiSrgRuBrcDJ7UwogJOAs4B9GZwF5ZlQkjSDxhYWVXUt8JQR7S+ZYp1VwKoR7WuBI3dpgZKkafMvuCVJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV1jC4sk+yRZk+SaJDckeWNrPzDJpUm+1u4PGFrn1CTrk9yc5NlD7Uclua7Ne1eSjKtuSdKOxrlncR/wi1X1ZGAZcGySY4BTgMuqailwWXtMksOBFcARwLHAu5MsaH2dDqwElrbbsWOsW5I0wdjCogbubQ8f3m4FHAec3drPBo5v08cB51fVfVV1C7AeODrJIcD+VXVlVRVwztA6kqQZMNZjFkkWJFkHbAIuraqrgIOr6k6Adv+Ytvgi4Pah1Te0tkVtemK7JGmGjDUsqmpbVS0DFjPYSzhyisVHHYeoKdp37CBZmWRtkrWbN2/e6XolSaPNyNlQVfUt4HIGxxruakNLtPtNbbENwKFDqy0GNrb2xSPaRz3PGVW1vKqWL1y4cFe+BEma18Z5NtTCJI9u0/sCzwK+AlwInNAWOwG4oE1fCKxIsneSwxgcyF7Thqq2JDmmnQX10qF1JEkzYK8x9n0IcHY7o+lhwOqq+kSSK4HVSU4EbgOeD1BVNyRZDdwIbAVOrqptra+TgLOAfYFL2k2SNEPGFhZVdS3wlBHtdwPPnGSdVcCqEe1rgamOd0iSxsi/4JYkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHWNLSySHJrks0luSnJDkle29tOS3JFkXbs9Z2idU5OsT3JzkmcPtR+V5Lo2711JMq66JUk72muMfW8FXlNVX06yH3B1kkvbvHdU1VuHF05yOLACOAJ4LPDpJD9RVduA04GVwBeBi4FjgUvGWLskacjY9iyq6s6q+nKb3gLcBCyaYpXjgPOr6r6qugVYDxyd5BBg/6q6sqoKOAc4flx1S5J2NCPHLJIsAZ4CXNWaXpHk2iRnJjmgtS0Cbh9abUNrW9SmJ7ZLkmbI2MMiySOBjwCvqqrvMBhSegKwDLgTeNv2RUesXlO0j3qulUnWJlm7efPmh1q6JKkZa1gkeTiDoDivqj4KUFV3VdW2qrofeC9wdFt8A3Do0OqLgY2tffGI9h1U1RlVtbyqli9cuHDXvhhJmsfGeTZUgPcDN1XV24faDxla7HnA9W36QmBFkr2THAYsBdZU1Z3AliTHtD5fClwwrrolSTsa59lQTwNeAlyXZF1r+yPgRUmWMRhKuhX4bYCquiHJauBGBmdSndzOhAI4CTgL2JfBWVCeCSVJM2hsYVFVX2D08YaLp1hnFbBqRPta4MhdV50kaWf4F9ySpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeqaVlgkedp02iRJe6bp7ln872m2SZL2QHtNNTPJU4GfBRYmefXQrP2BBeMsTJI0d0wZFsAjgEe25fYbav8O8N/HVZQkaW6ZMiyq6nPA55KcVVXf2JmOkxwKnAP8GHA/cEZV/VWSA4EPAUuAW4EXVNW/tHVOBU4EtgG/X1WfbO1HAWcB+wIXA6+sqtqZeiRJD950j1nsneSMJJ9K8pntt846W4HXVNWTgGOAk5McDpwCXFZVS4HL2mPavBXAEcCxwLuTbB/qOh1YCSxtt2On/xIlSQ9Vbxhquw8D7wHex+Bbf1dV3Qnc2aa3JLkJWAQcBzy9LXY2cDnwutZ+flXdB9ySZD1wdJJbgf2r6kqAJOcAxwOXTLN2SdJDNN2w2FpVpz/YJ0myBHgKcBVwcAsSqurOJI9piy0Cvji02obW9v02PbFdkjRDpjsM9fEkv5vkkCQHbr9NZ8UkjwQ+Aryqqr4z1aIj2mqK9lHPtTLJ2iRrN2/ePJ3yJEnTMN09ixPa/WuH2gr48alWSvJwBkFxXlV9tDXfleSQtldxCLCptW8ADh1afTGwsbUvHtG+g6o6AzgDYPny5R4Al6RdZFp7FlV12IhbLygCvB+4qarePjTrQh4InxOAC4baVyTZO8lhDA5kr2lDVluSHNP6fOnQOpKkGTCtPYskLx3VXlXnTLHa04CXANclWdfa/gh4C7A6yYnAbcDzW183JFkN3MjgTKqTq2r7wfSTeODU2Uvw4LYkzajpDkP9zND0PsAzgS8z+DuKkarqC4w+3kBbf9Q6q4BVI9rXAkdOs1ZJ0i42rbCoqt8bfpzkUcC5Y6lIkjTnPNhLlP8bg2MKkqR5YLrHLD7OA6erLgCeBKweV1GSpLlluscs3jo0vRX4RlVtmGxhSdKeZbqnzn4O+AqDK88eAPz7OIuSJM0t0/1PeS8A1jA4zfUFwFVJvES5JM0T0x2Gej3wM1W1CSDJQuDTwN+OqzBJ0twx3bOhHrY9KJq7d2JdSdJubrp7Fn+f5JPAB9vjFzL4J0SSpHmg9z+4n8jgkuKvTfLfgJ9j8FfZVwLnzUB9kqQ5oDeU9E5gC0BVfbSqXl1Vf8Bgr+Kd4y1NkjRX9MJiSVVdO7GxXatpyVgqkiTNOb2w2GeKefvuykIkSXNXLyy+lOTlExvb5cWvHk9JkqS5pnc21KuAjyX5dR4Ih+XAI4DnjbEuSdIcMmVYVNVdwM8meQYP/D+Ji6rqM2OvTJI0Z0z3/1l8FvjsmGuRJM1R/hW2JKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldYwuLJGcm2ZTk+qG205LckWRduz1naN6pSdYnuTnJs4faj0pyXZv3riQZV82SpNHGuWdxFnDsiPZ3VNWydrsYIMnhwArgiLbOu5MsaMufDqwElrbbqD4lSWM0trCoqiuAe6a5+HHA+VV1X1XdAqwHjk5yCLB/VV1ZVQWcAxw/loIlSZOajWMWr0hybRumOqC1LQJuH1pmQ2tb1KYntkuSZtBMh8XpwBOAZcCdwNta+6jjEDVF+0hJViZZm2Tt5s2bH2KpkqTtZjQsququqtpWVfcD7wWObrM2AIcOLboY2NjaF49on6z/M6pqeVUtX7hw4a4tXpLmsRkNi3YMYrvnAdvPlLoQWJFk7ySHMTiQvaaq7gS2JDmmnQX1UuCCmaxZkgR7javjJB8Eng4clGQD8CfA05MsYzCUdCvw2wBVdUOS1cCNwFbg5Kra1ro6icGZVfsCl7SbJGkGjS0squpFI5rfP8Xyq4BVI9rXAkfuwtIkSTvJv+CWJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1jS0skpyZZFOS64faDkxyaZKvtfsDhuadmmR9kpuTPHuo/agk17V570qScdUsSRptnHsWZwHHTmg7BbisqpYCl7XHJDkcWAEc0dZ5d5IFbZ3TgZXA0nab2KckaczGFhZVdQVwz4Tm44Cz2/TZwPFD7edX1X1VdQuwHjg6ySHA/lV1ZVUVcM7QOpKkGTLTxywOrqo7Adr9Y1r7IuD2oeU2tLZFbXpiuyRpBs2VA9yjjkPUFO2jO0lWJlmbZO3mzZt3WXGSNN/NdFjc1YaWaPebWvsG4NCh5RYDG1v74hHtI1XVGVW1vKqWL1y4cJcWLknz2UyHxYXACW36BOCCofYVSfZOchiDA9lr2lDVliTHtLOgXjq0jiRphuw1ro6TfBB4OnBQkg3AnwBvAVYnORG4DXg+QFXdkGQ1cCOwFTi5qra1rk5icGbVvsAl7SZJmkFjC4uqetEks545yfKrgFUj2tcCR+7C0iRJO2muHOCWJM1hhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXbMSFkluTXJdknVJ1ra2A5NcmuRr7f6AoeVPTbI+yc1Jnj0bNUvSfDabexbPqKplVbW8PT4FuKyqlgKXtcckORxYARwBHAu8O8mC2ShYkuaruTQMdRxwdps+Gzh+qP38qrqvqm4B1gNHz3x5kjR/zVZYFPCpJFcnWdnaDq6qOwHa/WNa+yLg9qF1N7Q2SdIM2WuWnvdpVbUxyWOAS5N8ZYplM6KtRi44CJ6VAI973OMeepWSJGCW9iyqamO73wR8jMGw0l1JDgFo95va4huAQ4dWXwxsnKTfM6pqeVUtX7hw4bjKl6R5Z8bDIsmPJtlv+zTwS8D1wIXACW2xE4AL2vSFwIokeyc5DFgKrJnZqiVpfpuNYaiDgY8l2f78f1NVf5/kS8DqJCcCtwHPB6iqG5KsBm4EtgInV9W2WahbkuatGQ+Lqvo68OQR7XcDz5xknVXAqjGXJkmaxFw6dVaSNEcZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR17TZhkeTYJDcnWZ/klNmuR5Lmk90iLJIsAP4a+GXgcOBFSQ6f3aokaf7YLcICOBpYX1Vfr6p/B84HjpvlmiRp3thdwmIRcPvQ4w2tTZI0A/aa7QKmKSPaaoeFkpXAyvbw3iQ3j7Wq+eMg4JuzXcRckL+Y7Qo0CT+jzS74jD5+VOPuEhYbgEOHHi8GNk5cqKrOAM6YqaLmiyRrq2r5bNchTcbP6PjtLsNQXwKWJjksySOAFcCFs1yTJM0bu8WeRVVtTfIK4JPAAuDMqrphlsuSpHljtwgLgKq6GLh4tuuYpxza01znZ3TMUrXDcWJJkn7I7nLMQpI0iwwLTcpLrGiuS3Jmkk1Jrp/tWvZ0hoVG8hIr2k2cBRw720XMB4aFJuMlVjTnVdUVwD2zXcd8YFhoMl5iRdIPGBaazLQusSJpfjAsNJlpXWJF0vxgWGgyXmJF0g8YFhqpqrYC2y+xchOw2kusaK5J8kHgSuA/JtmQ5MTZrmlP5V9wS5K63LOQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFNIsm2JOuSXJPky0l+9iH0dXmSXf4/opMsSfLiXd2vNJFhIU3uu1W1rKqeDJwKvHmcT9au9LuzlgCGhcbOsJCmZ3/gXwCSPDLJZW1v47okx7X2JUluSvLeJDck+VSSfYc7SfKwJGcneVN7fG+SP01yFfDUJLcmOajNW57k8jZ9WpJzk3wmydeSvLx1+Rbg59se0B/MyJbQvLTb/A9uaRbsm2QdsA9wCPCLrf17wPOq6jvtF/sXk2y/FMpS4EVV9fIkq4FfAz7Q5u0FnAdcX1WrWtuPtsf/CyAZdf3GH/gp4Ji2zj8muQg4BfjDqnruQ3610hQMC2ly362qZQBJngqck+RIBlfk/fMkvwDcz+DS7Qe3dW6pqnVt+moGw0Tb/V8Gl01ZNdS2DfjINOu5oKq+C3w3yWcZ/M+Rb+3ka5IeFIehpGmoqiuBg4CFwK+3+6NamNzFYO8D4L6h1bbxw1/I/gF4RpJ9htq+V1Xbhh5v5YGfy+HlYMdLxHutHs0Yw0KahiQ/CSwA7gYeBWyqqu8neQbw+Gl2837gYuDDSSbbq78VOKpN/9qEeccl2SfJfwCezuDKwFuA/ab7OqQHy7CQJrdvO3C8DvgQcELbCzgPWJ5kLYO9jK9Mt8OqejvwZeDcJKN+/t4I/FWSzzPYMxm2BrgI+CLwZ1W1EbgW2NpO7/UAt8bGq85Ku4EkpwH3VtVbZ7sWzU/uWUiSutyzkCR1uWchSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1PX/AQNFbJilAatJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Re-plotting class balance\n",
    "fig, ax = plt.subplots(figsize=(6, 6)) \n",
    "ax.bar([0,1],y_res.value_counts())\n",
    "ax.set_xlabel('Bankrupt')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_xticks([0,1])\n",
    "ax.set_title('Distribution of company bankruptcy ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is great about this library is that it is built to integrate with scikit learn, so we can just add the SMOTE transformer onto our Pipelines. The only caveat is that, to do this, we must import the `make_pipeline()` function from Imbalanced Learn (for documentation, see [here](https://imbalanced-learn.org/stable/references/generated/imblearn.pipeline.make_pipeline.html)). Have a go at changing our Pipelines (add SMOTE resampling as the first step) and then use these to predict on our validation samples:\n",
    "- Import the `make_pipeline()` function from imblearn.\n",
    "- Make a pipeline for each estimator by passing the following transformers (in order): SMOTE, Scaling, PCA, estimator.\n",
    "- Fit on the training set and predict on the validation set using all three pipelines.\n",
    "- Print out the classification report for all three models. \n",
    "\n",
    "> **Note:** The `.make_pipeline()` function, unlike the method `Pipeline()`, does not require a list of tuples as an argument. Instead, just pass in whatever transformers and estimators you want in order (separated by commas.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing make_pipeline\n",
    "from imblearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create three Pipelines, one for each model\n",
    "knn_pipe = make_pipeline(SMOTE(random_state=253), StandardScaler(), PCA(n_components=30), KNeighborsClassifier())\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and predict KNN pipeline\n",
    "# ...\n",
    "print('KNN Classification Report:')\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and predict SVM pipeline\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit and predict Logistic Regression pipeline\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping it all up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're nearly there! Now its time to train our models using the whole initial training set (training + validation) to then predict on the test set:\n",
    "- Merge the training and validation sets into a new training set (covered in previous sessions).\n",
    "- Fit each model on the new training set.\n",
    "- Predict with each model on the test set.\n",
    "- Print out the classification report of your predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging together training and validation samples\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train dimensions: ', X_train.shape)\n",
    "print('y_train dimensions: ', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and predict KNN pipeline\n",
    "knn_pipe.fit(X_train, y_train)\n",
    "y_pred = knn_pipe.predict(X_test)\n",
    "print('KNN Classification Report:')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and predict SVM pipeline\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit and predict Logistic Regression pipeline\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking it up a notch\n",
    "With this, we come to the end of this project, although this does not mean that you're done by any means. The project explored today can be improved in many ways to achieve better results, and doing so is what will truly help you grow as a data scientist. Here are some suggestions for improvements to this particular project:\n",
    "- **EDA:**\n",
    "    - How are the features in our data set distributed?\n",
    "    - What is the individual relationship of each of these with our target variable?\n",
    "    \n",
    "    \n",
    "- **Dimentionality reduction:**\n",
    "    - What number of components should we use to improve our predictions?\n",
    "    - What other dimensionality reduction techniques are there? (Hint: have a look at manifold learning models like [tSNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)) \n",
    "    \n",
    "    \n",
    "- **Preprocessing:**\n",
    "    - What other scaling methods are there? Which is the best for this problem?\n",
    "    - Is there any feature engineering that we can do? How?\n",
    "    - How do other resampling methods affect performance?\n",
    "    \n",
    "    \n",
    "- **Modelling:**\n",
    "    - What other classification models can you use?\n",
    "    - Try out ensemble models: random forests, LightGBM XGBoost... which works out best?\n",
    "    - Can you optimize any hyperparameters? How?\n",
    "    \n",
    "- **Evaluation Metrics:**\n",
    "    - Try looking at the ROC curve and the associated AUC metric [(link here)](https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5). Can you implement these in your project? \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression: What are the drivers of worker productivity?\n",
    "For this project, we will aim to predict the level of productivity for workers in the garment industry given a variety of production and supply chain metrics. To do this, we will use  data collected from January to March 2015 by the Industrial Engineering (IE) department of a garment manufacturing unit in\n",
    "Bangladesh. This kind of model would have all sorts of real world applications within the fields of operational research, labour economics, and supply chain management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data set overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start out, lets get an overview of our data set dimensions and feature types. We'll be doing a very simple overview in this section but feel free to perform more extensive EDA on your own: the more you know about your data, the better you can harness it through modelling.\n",
    "\n",
    "Start out with the following tasks:\n",
    "- Read in the data set as a pandas DataFrame (file path: `data/garments_worker_productivity.csv'`)\n",
    "- Output the first ten data set rows\n",
    "- Output the last ten data set rows\n",
    "- Print out a comprehensive summary of the data set (dimensions, variable names, data types, and null values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read bankruptcy data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the first ten rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the last ten rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the data set overview (dimensions, variable names, data types, null values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering with dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the features in our data set provides date information. In this format, the feature can be quite useless, but there are several things that we can do to extract useful information from it relating to worker productivity. To start, we want to convert the feature entries into `datetime` objects. To do this:\n",
    "- Use the `pd.to_datetime()` function to reset the `'date'` column in our data set.\n",
    "> **Note:** In order to get the right format, pass the argument `format = '%m/%d/%Y' `, and have a read [here](https://www.w3schools.com/python/python_datetime.asp) for further information on how to handle datetime objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting 'date' column into datetime \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our 'date' column in `datetime` format, we can now proceed to extract a number of additional features. Although you can find the full list of `datetime` attributes [here](https://pandas.pydata.org/pandas-docs/version/0.23/api.html#datetimelike-properties), we will focus on extracting the day of the week and the month as separate features. To extract a `datetime` attribute as a feature, we can use the following command:\n",
    "- `data['<new_feature_label>'] = data['date'].dt.<attribute>`\n",
    "\n",
    "Try extracting the attributes `.month` and `.dayofweek` and assigning it to columns with the labels `'month'` and `'day'` respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating month and day of the week features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we want to drop the `'date'` feature from our data set (as this can be hard to encode) and output the first ten rows of our data set and the data set overview to check the new features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping 'date' column\n",
    "data = data.drop(columns='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the first ten rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the data set overview (dimensions, variable names, data types, null values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've extracted some new features from our date data, we want to encode all our features so that they're ready for machine learning modeling. This includes encoding all of our categorical features as 'dummy variables'. Whilst this may seem easy at first sight, check the unique values (using `.unique()`) for the following features: `'team'`, `'month'`, `'day'`. Do these seem like continuous or categorical variables to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking unique values for 'team'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking unique values for 'month' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking unique values for 'day' \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, these features, although of numerical data type, fall much better under the definition of categorical variables. Had we not noticed, they would not get dummy encoded by the `pd.get_dummies()` function, leading to the incorrect handling of our data. Thus, its important to always check what kind of features we have, irrespective if data type. We can get around this encoding problem by casting these features as `str` types and then proceeding with the dummy encoding. In our data set:\n",
    "- Cast the features `'team'`, `'month'`, and `'day'` as strings.\n",
    "- Use the `pd.get_dummies()` function to dummy encode categorical variables (Hint: remember to drop one dummy pre categorical feature using the `drop_first` parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast 'team', 'month', and 'day' features as str\n",
    "data['team'] = data['team'].astype(str)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dummy encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print data set overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our ML setup\n",
    "Now that we have our variables encoded we want to set up our data in the usual machine learning configuration: a training and a test set, each with a feature matrix and a target vector. To start:\n",
    "\n",
    "- Import the `train_test_split()` function from scikit learn\n",
    "- Declare a target vector y (corresponding to the `actual_productivity` column in the data set)\n",
    "- Declare a feature matrix X (including all columns except `actual_productivity`)\n",
    "- Create a training and test set\n",
    "- Print the dimensions of X and y in both the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing train-test split function\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the training vector y which has the column label 'actual_productivity'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare feature matrix \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a train-test- split using random state 253, test_size 30%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print dimensions of the training set\n",
    "print('X_train dimensions: ', X_train.shape)\n",
    "print('y_train dimensions: ', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print dimensions of the test set\n",
    "print('X_test dimensions: ', X_test.shape)\n",
    "print('y_test dimensions: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing value imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you probably already noticed, we have several missing values in the `'wip'` column of our data set. This time, however, we will use a more advanced method to impute missing values. More specifically, we will use the K Nearest Neighbors algorithm to impute values that are similar to other nearby samples, which should hopefully give us more accuracy. To do this:\n",
    "- Import the `KNNImputer` class from scikit learn\n",
    "- Create `KNNImputer` transformer, passing `n_neighbors=7` as an argument.\n",
    "- Fit the transformer using `X_train`\n",
    "- Impute missing values by calling the `.transform()` method with `X_train` as an argument. Assign this to a new variable `X_full`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing KNNImputer\n",
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating KNNImputer transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting and transforming training feature matrix using our imputer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might've noticed that most machine learning with scikit learn involves:\n",
    "- Instantiating a transformer or model.\n",
    "- Fitting this to our data using `.fit()`.\n",
    "- Transforming or predicting using `.transform()` or `.predict()`.\n",
    "\n",
    "This simple, general process means that we can simplify our ML process using **Pipelines**. A Pipeline allows you to establish a sequence of transformations terminating with *one estimator*. Pipelines can then be treated as any other scikit learn estimator, but each time they perform all of the interim transformation steps in the sequence. This has the benefit of greatly simplifying your code and can also help prevent [data leakage](https://machinelearningmastery.com/data-leakage-machine-learning/). \n",
    "\n",
    "To instantiate a Pipeline you must pass as an argument a list of $n+1$ tuples (where $n$ is the number of transforms) in the form: `[('transformer_1_name', transformer_1), ..., ('estimator_name', estimator)]`.\n",
    "\n",
    "We will now create Pipelines for both an SVM and a Linear Regression model below:\n",
    "- Import the `StandardScaler`, `SVR`, and `LinearRegression` classes from scikit learn.\n",
    "- Import the `Pipeline` class from scikit learn.\n",
    "- For each one of the two models, create a Pipeline with the following transforms (in order): imputation, scaling, estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing KNN, SVM, and Linear Regression models \n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Pipeline\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine these into two Pipelines, one for each model\n",
    "lin_reg_pipe = Pipeline([('Imputation', KNNImputer(n_neighbors=7)), \n",
    "                         ('Scaler', StandardScaler()), \n",
    "                         ('Model', LinearRegression())])\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to see how well our models stack up! For regression problems, one metric that is often used to assess model performance is the **Root Mean Squared Error (RMSE)**. this is given by:\n",
    "$$\\text{RMSE}=\\sqrt{\\frac{\\sum_{i=1}^n (\\,y_i\\,-\\,y_i^{pred}\\,)^2}{n}}$$\n",
    "\n",
    "We can compare our models by computing the cross-validated RMSE scores:\n",
    "- Import the `cross_val_score()` function from scikit learn.\n",
    "- Calculate the cross-validated scores for each pipeline estimator. \n",
    "- Print the mean score for each pipeline estimator.\n",
    "> **Note:**  Pass `scoring = 'neg_root_mean_squared_error'` as an argument to `cross_val_score()` to ensure that it computes the RMSE for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the cross-validated score of these two models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Linear Regression RMSE Score:')\n",
    "print((-1)*lin_reg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SVM RMSE Score:')\n",
    "print((-1)*svm_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now learn about a more advanced technique called **regularization** that can help us build better regression models. Regularization is an adaptation on the standard linear regression and essentially works by placing a penalty on the size of the regression coefficients when fitting. This is because large coefficients tend to overfit our model to the training data, so by shrinking them we enable the model to generalize better and improve performance on unseen data. A more detailed description of regularization can be found [here](https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a). \n",
    "\n",
    "We will focus on one particular kind of regularized regression model known as **Ridge regression**. One key aspect of ridge regression is the `alpha` hyperparameter, which is used to denote the strength of the penalty on coefficient size. A larger value of`alpha` will seek to shrink coefficients more and viseversa. Finding the optimal `alpha` value can be cumbersome, but luckily we can use the hyperparameter optimization techniques learned in Session Five to do this. \n",
    "\n",
    "Lets try out using a Ridge model for our productivity problem:\n",
    "- Import the `Ridge` and `GridSearchCV` classes from scikit learn\n",
    "- Create a Pipeline with the following transforms (in order): imputation, scaling, Ridge estimator.\n",
    "- Create a `GridSearchCV` object by passing our Ridge pipeline, and the dictionary `params`.\n",
    "- Fit the `GridSearchCV` object using `X_train` and `y_train`.\n",
    "- Print the optimal `alpha` value using the `.best_params_` attribute on our `GridSearchCV` object.\n",
    "- Print the best RMSE score using the `.best_score_` attribute on our `GridSearchCV` object.\n",
    "- Assign the optimized model to `ridge_pipe` using the `.best_estimator_` attribute on our `GridSearchCV` object.\n",
    "> **Note:**  Pass `scoring = 'neg_root_mean_squared_error'` as an argument to `GridSearchCV()` to ensure that it computes the RMSE for our Ridge model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Ridge\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Ridge Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'Model__alpha':np.linspace(0, 1, 40)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizing alpha hyperparameter using GridSearchCV\n",
    "grid_search = #...\n",
    "print('Best Alpha: ', grid_search.best_params_)\n",
    "print('RMSE: ', grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving optimized Ridge model\n",
    "ridge_pipe = #..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping it all up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're nearly there! Now its time to train our models using the whole initial training set to then predict on the test set:\n",
    "- Import the `mean_squared_error()` function from scikit learn\n",
    "- Fit each model on the training set.\n",
    "- Predict with each model on the test set.\n",
    "- Print out the RMSE of your predictions using mean_squared_error(). \n",
    "\n",
    "> **Note**: To get the RMSE instead of the MSE, just pass `squared=False` as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing mean_squared_error\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and predict Linear Regression pipeline\n",
    "#...\n",
    "print('Linear Regression RMSE:')\n",
    "print(mean_squared_error(y_test, y_pred, squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and predict SVM pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and predict Ridge pipeline\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
